{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Raw Data (Preparando conjunto de dados)\n",
    "\n",
    "This is necessary to try any subsection of this Notebook, so execute it!\n",
    "\n",
    "---\n",
    "\n",
    "Essa parte é necessária para executar qualquer subseção desse Notebook, portanto, é necessário executá-la sempre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTED_MODES = np.arange(1, 33, dtype=int)\n",
    "TESTED_MODES_FULL_ORDER = np.arange(1, 100, 5, dtype=int)\n",
    "TRAIN_SPLIT = 2/3\n",
    "# Kernels disponíveis para uso pelo SKLearn\n",
    "USED_KERNEL: Literal['linear', 'poly', 'rbf', 'sigmoid'] = 'rbf'\n",
    "kernel_name = 'radial basis function'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_file = np.load(\".data/Time_series_colisao.npz\")\n",
    "display(collision_file)\n",
    "spreading_file = np.load(\".data/Time_series_espalhamento.npz\")\n",
    "display(spreading_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_timesteps_raw = collision_file['TS']\n",
    "display(f\"Dimensions of the collision simulations: {collision_timesteps_raw.shape}\")\n",
    "display(f\"That is, {collision_timesteps_raw.shape[0]} simulations, with {collision_timesteps_raw.shape[1]} timesteps each, having {collision_timesteps_raw.shape[2]} components each.\")\n",
    "spreading_timesteps_raw = spreading_file['TS']\n",
    "display(f\"Dimensions of the spreading simulations: {spreading_timesteps_raw.shape}\")\n",
    "display(f\"That is, {spreading_timesteps_raw.shape[0]} simulations, with {spreading_timesteps_raw.shape[1]} timesteps each, having {spreading_timesteps_raw.shape[2]} components each.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collision component-wise order reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reordering data (reordenação de dados)\n",
    "\n",
    "We want to avoid mixing components in this case\n",
    "\n",
    "---\n",
    "\n",
    "Precisamos separar as componentes nesse caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_names = ['Diameter', 'Height', 'Kinetic Energy', 'Surface Energy', 'Dissipative Energy']\n",
    "components_names_pt = ['Diâmetro', 'Altura', 'Energia Cinética', 'Energia Superficial', 'Energia Dissipativa']\n",
    "components_names_pt_treated = ['diametro', 'altura', 'energia_cinetica', 'energia_superficial', 'energia_dissipativa']\n",
    "collision: dict[str, Any] = dict()\n",
    "for i, cn in enumerate(components_names):\n",
    "    cn_dict = {}\n",
    "    cn_dict['original_data'] = collision_timesteps_raw[:, :, i].T\n",
    "    cn_dict['pt_name'] = components_names_pt[i]\n",
    "    cn_dict['pt_name_treated'] = components_names_pt_treated[i]\n",
    "    # (n_simulations, timesteps) [RAW]\n",
    "    # (timesteps, n_simulations) [.T]\n",
    "    collision[cn] = cn_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Firstly, we want to remove the \"temporal mean\", that is, the mean for each simulation of each component. After that, we can also scale the data so its standard variation is equal to 1:\n",
    "\n",
    "$$\\pmb{\\hat{X}}=\\sqrt{\\frac{1}{n}}\\left(\\pmb{X}-\\hat{x}1\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "Aplica-se uma estandardização para que a média e desvio padrão de cada coluna seja igual a 0 e 1, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for key, obj in collision.items():\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(obj['original_data'])\n",
    "    display(f\"Asserting mean = 0 and std = 1 for component '{key}'\")\n",
    "    assert np.allclose(scaled_data.mean(axis=0), 0)\n",
    "    assert np.allclose(scaled_data.std(axis=0), 1)\n",
    "    display(\"Success!\")\n",
    "    obj['scaled_data'] = scaled_data\n",
    "    obj['scaler'] = scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for all the given modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "NUMBER_OF_MODES = TESTED_MODES.shape[0]\n",
    "for key, obj in collision.items():\n",
    "    number_of_timesteps = obj['scaled_data'].shape[0]\n",
    "    permutation_idx = np.random.permutation(number_of_timesteps)\n",
    "    training_size = int(TRAIN_SPLIT * number_of_timesteps)\n",
    "    training_idx = permutation_idx[:training_size]\n",
    "    testing_idx = permutation_idx[training_size:]\n",
    "    training_data = obj['original_data'][training_idx]\n",
    "    training_data_scaled = obj['scaled_data'][training_idx]\n",
    "    testing_data = obj['original_data'][testing_idx]\n",
    "    testing_data_scaled = obj['scaled_data'][testing_idx]\n",
    "    modes_mse = np.zeros((number_of_timesteps, NUMBER_OF_MODES))\n",
    "    modes_mae = np.zeros_like(modes_mse)\n",
    "    modes_mrae = np.zeros_like(modes_mse)\n",
    "    for i, mode in enumerate(TESTED_MODES):\n",
    "        # TRAINING DATA\n",
    "        kpca = KernelPCA(n_components=mode, random_state=42, kernel=USED_KERNEL, fit_inverse_transform=True)\n",
    "        kpca.fit(training_data_scaled)\n",
    "        reconstructed_training_scaled = kpca.inverse_transform(kpca.transform(training_data_scaled))\n",
    "        reconstructed_training = obj['scaler'].inverse_transform(reconstructed_training_scaled)\n",
    "        mse_training = np.mean((training_data - reconstructed_training)**2, axis=1)\n",
    "        mae_training = np.mean(np.abs(training_data - reconstructed_training), axis=1)\n",
    "        mrae_training = np.mean(np.abs(training_data - reconstructed_training) / np.mean(training_data), axis=1)\n",
    "        # TESTING DATA\n",
    "        reconstructed_testing_scaled = kpca.inverse_transform(kpca.transform(testing_data_scaled))\n",
    "        reconstructed_testing = obj['scaler'].inverse_transform(reconstructed_testing_scaled)\n",
    "        mse_testing = np.mean((testing_data - reconstructed_testing)**2, axis=1)\n",
    "        mae_testing = np.mean(np.abs(testing_data - reconstructed_testing), axis=1)\n",
    "        mrae_testing = np.mean(np.abs(testing_data - reconstructed_testing) / np.abs(testing_data), axis=1)\n",
    "        # STORING DATA\n",
    "        modes_mse[training_idx, i] = mse_training\n",
    "        modes_mae[training_idx, i] = mae_training\n",
    "        modes_mrae[training_idx, i] = mrae_training\n",
    "        modes_mse[testing_idx, i] = mse_testing\n",
    "        modes_mae[testing_idx, i] = mae_testing\n",
    "        modes_mrae[testing_idx, i] = mrae_testing\n",
    "    training_error_idx = pd.Index(training_idx, name='Timestep')\n",
    "    testing_error_idx = pd.Index(testing_idx, name='Timestep')\n",
    "    columns = pd.MultiIndex.from_product([TESTED_MODES, ['MSE', 'MAE', 'MRAE']], names=['Modes', 'Metric'])\n",
    "    training_error_data = np.zeros((training_error_idx.shape[0], NUMBER_OF_MODES * 3))\n",
    "    testing_error_data = np.zeros((testing_error_idx.shape[0], NUMBER_OF_MODES * 3))\n",
    "    training_error_data[:, ::3] = modes_mse[training_idx]\n",
    "    training_error_data[:, 1::3] = modes_mae[training_idx]\n",
    "    training_error_data[:, 2::3] = modes_mrae[training_idx]\n",
    "    testing_error_data[:, ::3] = modes_mse[testing_idx]\n",
    "    testing_error_data[:, 1::3] = modes_mae[testing_idx]\n",
    "    testing_error_data[:, 2::3] = modes_mrae[testing_idx]\n",
    "    training_error_df = pd.DataFrame(training_error_data, columns=columns, index=training_error_idx).sort_index()\n",
    "    testing_error_df = pd.DataFrame(testing_error_data, columns=columns, index=testing_error_idx).sort_index()\n",
    "    obj['permutation_idx'] = permutation_idx\n",
    "    obj['training_idx'] = training_idx\n",
    "    obj['testing_idx'] = testing_idx\n",
    "    obj['training_error'] = training_error_df\n",
    "    obj['testing_error'] = testing_error_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing these errors in graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = f\".results/kpca/{USED_KERNEL}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib.ticker import MaxNLocator, PercentFormatter\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "os.makedirs(BASE_PATH, exist_ok=True)\n",
    "\n",
    "for key, obj in collision.items():\n",
    "    training_error = obj['training_error'].loc[:,\n",
    "                                               (slice(None), 'MRAE')].mean().unstack() * 100\n",
    "    testing_error = obj['testing_error'].loc[:,\n",
    "                                             (slice(None), 'MRAE')].mean().unstack() * 100\n",
    "    # region COMPONENT-SPECIFIC FIGURE\n",
    "    component_fig, component_ax = plt.subplots(1, 2, figsize=(16, 9))\n",
    "    training_error.plot(ax=component_ax[0], linestyle='--', alpha=0.5)\n",
    "    testing_error.plot(ax=component_ax[0], linestyle='-')\n",
    "    component_ax[0].set_ylabel(\"Erro médio absoluto percentual (MAPE)\")\n",
    "    component_ax[0].set_xlabel(\"Número de modos\")\n",
    "    component_ax[1].remove()\n",
    "    component_ax[1] = component_fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    x, y = np.meshgrid(obj['testing_error'].index, TESTED_MODES)\n",
    "    z = obj['testing_error'].loc[:, (slice(None), 'MRAE')].values.T * 100\n",
    "    component_ax[1].plot_surface(x, y, z)\n",
    "    component_ax[1].set_xlabel(\"Snapshot\")\n",
    "    component_ax[1].set_ylabel(\"Número de modos\")\n",
    "    component_ax[1].set_zlabel(\"Erro médio absoluto percentual (MAPE)\")\n",
    "    component_ax[1].set_title(\"Erro de teste\")\n",
    "    max_error_last_mode_train = obj[\"training_error\"].loc[:,\n",
    "                                                          (TESTED_MODES[-1], 'MRAE')].max() * 100\n",
    "    max_error_last_mode_test = obj[\"testing_error\"].loc[:,\n",
    "                                                        (TESTED_MODES[-1], 'MRAE')].max() * 100\n",
    "    max_error_last_modes = max(\n",
    "        max_error_last_mode_train, max_error_last_mode_test)\n",
    "    component_ax[0].scatter(\n",
    "        [TESTED_MODES[-1]], [max_error_last_modes], color='red')\n",
    "    component_ax[0].annotate(f\"{max_error_last_modes:.2f}%\", (TESTED_MODES[-1],\n",
    "                             max_error_last_modes), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "    component_ax[0].legend(\n",
    "        ['Treinamento', 'Teste', 'Erro máximo no melhor caso'])\n",
    "    component_ax[0].yaxis.set_major_locator(MaxNLocator())\n",
    "    component_ax[0].yaxis.set_major_formatter(PercentFormatter())\n",
    "    component_ax[1].zaxis.set_major_locator(MaxNLocator())\n",
    "    component_ax[1].zaxis.set_major_formatter(PercentFormatter())\n",
    "    component_fig.suptitle(\n",
    "        f\"Erro médio absoluto percentual para a componente '{obj[\"pt_name\"]}'\")\n",
    "    os.makedirs(f\"{BASE_PATH}/collision/separate\", exist_ok=True)\n",
    "    component_fig.suptitle(f\"\"\"Simulação da colisão\n",
    "                           Kernel PCA com kernel {kernel_name}: Redução da componente '{obj[\"pt_name\"]}'\"\"\")\n",
    "    component_fig.tight_layout()\n",
    "    component_fig.savefig(\n",
    "        f\"{BASE_PATH}/collision/separate/{obj[\"pt_name_treated\"]}.png\", dpi=333, bbox_inches='tight')\n",
    "    # endregion\n",
    "    training_error.plot(ax=ax, linestyle='--', alpha=0.5)\n",
    "    testing_error.plot(ax=ax, linestyle='-')\n",
    "\n",
    "training_legend = [\n",
    "    f\"{cn[\"pt_name\"]} (treinamento)\" for cn in collision.values()]\n",
    "testing_legend = [f\"{cn[\"pt_name\"]} (teste)\" for cn in collision.values()]\n",
    "legend = [''] * (len(training_legend) + len(testing_legend))\n",
    "legend[::2] = training_legend\n",
    "legend[1::2] = testing_legend\n",
    "ax.legend(legend)\n",
    "ax.yaxis.set_major_locator(MaxNLocator())\n",
    "ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "ax.set_title(f\"\"\"Simulação da colisão\n",
    "             Kernel PCA com kernel {kernel_name}: Componentes separadas\"\"\")\n",
    "ax.set_xlabel(\"Número de modos\")\n",
    "ax.set_ylabel(\"Erro médio absoluto percentual (MAPE)\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{BASE_PATH}/collision/separate/todos.png\", dpi=333, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spreading component-wise order reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reordering data (reordenação de dados)\n",
    "\n",
    "We want to avoid mixing components in this case\n",
    "\n",
    "---\n",
    "\n",
    "Precisamos separar as componentes nesse caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreading_components_names = ['Diameter', 'Kinetic Energy', 'Surface Energy', 'Dissipative Energy']\n",
    "spreading_components_names_pt = ['Diâmetro', 'Energia Cinética', 'Energia Superficial', 'Energia Dissipativa']\n",
    "spreading_components_names_pt_treated = ['diametro', 'energia_cinetica', 'energia_superficial', 'energia_dissipativa']\n",
    "spreading = dict()\n",
    "for i, cn in enumerate(spreading_components_names):\n",
    "    cn_dict = {}\n",
    "    cn_dict['original_data'] = spreading_timesteps_raw[:, :, i].T\n",
    "    cn_dict['pt_name'] = spreading_components_names_pt[i]\n",
    "    cn_dict['pt_name_treated'] = spreading_components_names_pt_treated[i]\n",
    "    # (n_simulations, timesteps) [RAW]\n",
    "    # (timesteps, n_simulations) [.T]\n",
    "    spreading[cn] = cn_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Firstly, we want to remove the \"temporal mean\", that is, the mean for each simulation of each component. After that, we can also scale the data so its standard variation is equal to 1:\n",
    "\n",
    "$$\\pmb{\\hat{X}}=\\sqrt{\\frac{1}{n}}\\left(\\pmb{X}-\\hat{x}1\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "Aplica-se uma estandardização para que a média e desvio padrão de cada coluna seja igual a 0 e 1, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for key, obj in spreading.items():\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(obj['original_data'])\n",
    "    display(f\"Asserting mean = 0 and std = 1 for component '{key}'\")\n",
    "    assert np.allclose(scaled_data.mean(axis=0), 0)\n",
    "    assert np.allclose(scaled_data.std(axis=0), 1)\n",
    "    display(\"Success!\")\n",
    "    obj['scaled_data'] = scaled_data\n",
    "    obj['scaler'] = scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for all the given modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "NUMBER_OF_MODES = TESTED_MODES.shape[0]\n",
    "for key, obj in spreading.items():\n",
    "    number_of_timesteps = obj['scaled_data'].shape[0]\n",
    "    permutation_idx = np.random.permutation(number_of_timesteps)\n",
    "    training_size = int(TRAIN_SPLIT * number_of_timesteps)\n",
    "    training_idx = permutation_idx[:training_size]\n",
    "    testing_idx = permutation_idx[training_size:]\n",
    "    training_data = obj['original_data'][training_idx]\n",
    "    training_data_scaled = obj['scaled_data'][training_idx]\n",
    "    testing_data = obj['original_data'][testing_idx]\n",
    "    testing_data_scaled = obj['scaled_data'][testing_idx]\n",
    "    modes_mse = np.zeros((number_of_timesteps, NUMBER_OF_MODES))\n",
    "    modes_mae = np.zeros_like(modes_mse)\n",
    "    modes_mrae = np.zeros_like(modes_mse)\n",
    "    for i, mode in enumerate(TESTED_MODES):\n",
    "        # TRAINING DATA\n",
    "        kpca = KernelPCA(n_components=mode, random_state=42, kernel=USED_KERNEL, fit_inverse_transform=True)\n",
    "        kpca.fit(training_data_scaled)\n",
    "        reconstructed_training_scaled = kpca.inverse_transform(kpca.transform(training_data_scaled))\n",
    "        reconstructed_training = obj['scaler'].inverse_transform(reconstructed_training_scaled)\n",
    "        mse_training = np.mean((training_data - reconstructed_training)**2, axis=1)\n",
    "        mae_training = np.mean(np.abs(training_data - reconstructed_training), axis=1)\n",
    "        mrae_training = np.mean(np.abs(training_data - reconstructed_training) / np.abs(training_data), axis=1)\n",
    "        # TESTING DATA\n",
    "        reconstructed_testing_scaled = kpca.inverse_transform(kpca.transform(testing_data_scaled))\n",
    "        reconstructed_testing = obj['scaler'].inverse_transform(reconstructed_testing_scaled)\n",
    "        mse_testing = np.mean((testing_data - reconstructed_testing)**2, axis=1)\n",
    "        mae_testing = np.mean(np.abs(testing_data - reconstructed_testing), axis=1)\n",
    "        mrae_testing = np.mean(np.abs(testing_data - reconstructed_testing) / np.abs(testing_data), axis=1)\n",
    "        # STORING DATA\n",
    "        modes_mse[training_idx, i] = mse_training\n",
    "        modes_mae[training_idx, i] = mae_training\n",
    "        modes_mrae[training_idx, i] = mrae_training\n",
    "        modes_mse[testing_idx, i] = mse_testing\n",
    "        modes_mae[testing_idx, i] = mae_testing\n",
    "        modes_mrae[testing_idx, i] = mrae_testing\n",
    "    training_error_idx = pd.Index(training_idx, name='Timestep')\n",
    "    testing_error_idx = pd.Index(testing_idx, name='Timestep')\n",
    "    columns = pd.MultiIndex.from_product([TESTED_MODES, ['MSE', 'MAE', 'MRAE']], names=['Modes', 'Metric'])\n",
    "    training_error_data = np.zeros((training_error_idx.shape[0], NUMBER_OF_MODES * 3))\n",
    "    testing_error_data = np.zeros((testing_error_idx.shape[0], NUMBER_OF_MODES * 3))\n",
    "    training_error_data[:, ::3] = modes_mse[training_idx]\n",
    "    training_error_data[:, 1::3] = modes_mae[training_idx]\n",
    "    training_error_data[:, 2::3] = modes_mrae[training_idx]\n",
    "    testing_error_data[:, ::3] = modes_mse[testing_idx]\n",
    "    testing_error_data[:, 1::3] = modes_mae[testing_idx]\n",
    "    testing_error_data[:, 2::3] = modes_mrae[testing_idx]\n",
    "    training_error_df = pd.DataFrame(training_error_data, columns=columns, index=training_error_idx).sort_index()\n",
    "    testing_error_df = pd.DataFrame(testing_error_data, columns=columns, index=testing_error_idx).sort_index()\n",
    "    obj['permutation_idx'] = permutation_idx\n",
    "    obj['training_idx'] = training_idx\n",
    "    obj['testing_idx'] = testing_idx\n",
    "    obj['training_error'] = training_error_df\n",
    "    obj['testing_error'] = testing_error_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing these errors in graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib.ticker import MaxNLocator, PercentFormatter\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "for key, obj in spreading.items():\n",
    "    training_error = obj['training_error'].loc[:,\n",
    "                                               (slice(None), 'MRAE')].mean().unstack() * 100\n",
    "    testing_error = obj['testing_error'].loc[:,\n",
    "                                             (slice(None), 'MRAE')].mean().unstack() * 100\n",
    "    # region COMPONENT-SPECIFIC FIGURE\n",
    "    component_fig, component_ax = plt.subplots(1, 2, figsize=(16, 9))\n",
    "    training_error.plot(ax=component_ax[0], linestyle='--', alpha=0.5)\n",
    "    testing_error.plot(ax=component_ax[0], linestyle='-')\n",
    "    component_ax[0].set_ylabel(\"Erro médio absoluto percentual (MAPE)\")\n",
    "    component_ax[0].set_xlabel(\"Número de modos\")\n",
    "    component_ax[1].remove()\n",
    "    component_ax[1] = component_fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    x, y = np.meshgrid(obj['testing_error'].index, TESTED_MODES)\n",
    "    z = obj['testing_error'].loc[:, (slice(None), 'MRAE')].values.T * 100\n",
    "    component_ax[1].plot_surface(x, y, z)\n",
    "    component_ax[1].set_xlabel(\"Snapshot\")\n",
    "    component_ax[1].set_ylabel(\"Número de modos\")\n",
    "    component_ax[1].set_zlabel(\"Erro médio absoluto percentual (MAPE)\")\n",
    "    component_ax[1].set_title(\"Erro de teste\")\n",
    "    max_error_last_mode_train = obj[\"training_error\"].loc[:,\n",
    "                                                          (TESTED_MODES[-1], 'MRAE')].max() * 100\n",
    "    max_error_last_mode_test = obj[\"testing_error\"].loc[:,\n",
    "                                                        (TESTED_MODES[-1], 'MRAE')].max() * 100\n",
    "    max_error_last_modes = max(\n",
    "        max_error_last_mode_train, max_error_last_mode_test)\n",
    "    component_ax[0].scatter(\n",
    "        [TESTED_MODES[-1]], [max_error_last_modes], color='red')\n",
    "    component_ax[0].annotate(f\"{max_error_last_modes:.2f}%\", (TESTED_MODES[-1],\n",
    "                             max_error_last_modes), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "    component_ax[0].legend(\n",
    "        ['Treinamento', 'Teste', 'Erro máximo no melhor caso'])\n",
    "    component_ax[0].yaxis.set_major_locator(MaxNLocator())\n",
    "    component_ax[0].yaxis.set_major_formatter(PercentFormatter())\n",
    "    component_ax[1].zaxis.set_major_locator(MaxNLocator())\n",
    "    component_ax[1].zaxis.set_major_formatter(PercentFormatter())\n",
    "    component_fig.suptitle(\n",
    "        f\"Erro médio absoluto percentual para a componente '{obj[\"pt_name\"]}'\")\n",
    "    component_fig.tight_layout()\n",
    "    os.makedirs(f\"{BASE_PATH}/spreading/separate\", exist_ok=True)\n",
    "    component_fig.suptitle(f\"\"\"Simulação do espalhamento\n",
    "                           Kernel PCA com kernel {kernel_name}: Redução da componente '{obj[\"pt_name\"]}'\"\"\")\n",
    "    component_fig.subplots_adjust(top=0.9)\n",
    "    component_fig.savefig(\n",
    "        f\"{BASE_PATH}/spreading/separate/{obj[\"pt_name_treated\"]}.png\", dpi=333, bbox_inches='tight')\n",
    "    # endregion\n",
    "    training_error.plot(ax=ax, linestyle='--', alpha=0.5)\n",
    "    testing_error.plot(ax=ax, linestyle='-')\n",
    "\n",
    "training_legend = [\n",
    "    f\"{cn[\"pt_name\"]} (treinamento)\" for cn in collision.values()]\n",
    "testing_legend = [f\"{cn[\"pt_name\"]} (teste)\" for cn in collision.values()]\n",
    "legend = [''] * (len(training_legend) + len(testing_legend))\n",
    "legend[::2] = training_legend\n",
    "legend[1::2] = testing_legend\n",
    "ax.legend(legend)\n",
    "ax.yaxis.set_major_locator(MaxNLocator())\n",
    "ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "ax.set_title(f\"\"\"Simulação de espalhamento\n",
    "            Kernel PCA com kernel {kernel_name}: Componentes separadas\"\"\")\n",
    "ax.set_xlabel(\"Número de modos\")\n",
    "ax.set_ylabel(\"Erro médio absoluto percentual (MAPE)\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{BASE_PATH}/spreading/separate/todos.png\", dpi=333)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collision full order reduction\n",
    "\n",
    "\"Full order reduction\" is the case when there are no separate components. In this case, every component is present in the dimensionality reduction.\n",
    "\n",
    "---\n",
    "\n",
    "Redução de dimensionalidade \"full order\" é o nome que iremos dar a reduções de dimensionalidade em que todas as componentes estão presentes, isto é, sem separação de componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reordering data and rescaling energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_timesteps_components_last = np.transpose(collision_timesteps_raw, (1, 0, 2))\n",
    "display(f\"Shape after transpose is: {collision_timesteps_components_last.shape}\")\n",
    "# Flattening 2nd and 3rd dimensions\n",
    "collision_timesteps_components_last = (\n",
    "    collision_timesteps_components_last\n",
    "    .reshape(collision_timesteps_components_last.shape[0], -1)\n",
    ")\n",
    "display(f\"Shape after flattening is: {collision_timesteps_components_last.shape}\")\n",
    "collision_full = {}\n",
    "collision_full['original_data'] = collision_timesteps_components_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(collision_full['original_data'])\n",
    "display(f\"Asserting mean = 0 and std = 1 for every column\")\n",
    "assert np.allclose(scaled_data.mean(axis=0), 0)\n",
    "assert np.allclose(scaled_data.std(axis=0), 1)\n",
    "display(\"Success!\")\n",
    "collision_full['scaled_data'] = scaled_data\n",
    "collision_full['scaler'] = scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing every mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "NUMBER_OF_MODES = TESTED_MODES_FULL_ORDER.shape[0]\n",
    "number_of_timesteps = collision_full['scaled_data'].shape[0]\n",
    "permutation_idx = np.random.permutation(number_of_timesteps)\n",
    "training_size = int(TRAIN_SPLIT * number_of_timesteps)\n",
    "training_idx = permutation_idx[:training_size]\n",
    "testing_idx = permutation_idx[training_size:]\n",
    "training_data = collision_full['original_data'][training_idx]\n",
    "training_data_scaled = collision_full['scaled_data'][training_idx]\n",
    "testing_data = collision_full['original_data'][testing_idx]\n",
    "testing_data_scaled = collision_full['scaled_data'][testing_idx]\n",
    "modes_mse = np.zeros((number_of_timesteps, NUMBER_OF_MODES))\n",
    "modes_mae = np.zeros_like(modes_mse)\n",
    "modes_mrae = np.zeros_like(modes_mse)\n",
    "for i, mode in enumerate(TESTED_MODES_FULL_ORDER):\n",
    "    # TRAINING DATA\n",
    "    kpca = KernelPCA(n_components=mode, random_state=42, kernel=USED_KERNEL, fit_inverse_transform=True)\n",
    "    kpca.fit(training_data_scaled)\n",
    "    reconstructed_training_scaled = kpca.inverse_transform(kpca.transform(training_data_scaled))\n",
    "    reconstructed_training = collision_full['scaler'].inverse_transform(reconstructed_training_scaled)\n",
    "    mse_training = np.mean((training_data - reconstructed_training)**2, axis=1)\n",
    "    mae_training = np.mean(np.abs(training_data - reconstructed_training), axis=1)\n",
    "    mrae_training = np.mean(np.abs(training_data - reconstructed_training) / np.abs(training_data), axis=1)\n",
    "    # TESTING DATA\n",
    "    reconstructed_testing_scaled = kpca.inverse_transform(kpca.transform(testing_data_scaled))\n",
    "    reconstructed_testing = collision_full['scaler'].inverse_transform(reconstructed_testing_scaled)\n",
    "    mse_testing = np.mean((testing_data - reconstructed_testing)**2, axis=1)\n",
    "    mae_testing = np.mean(np.abs(testing_data - reconstructed_testing), axis=1)\n",
    "    mrae_testing = np.mean(np.abs(testing_data - reconstructed_testing) / np.abs(testing_data), axis=1)\n",
    "    # STORING DATA\n",
    "    modes_mse[training_idx, i] = mse_training\n",
    "    modes_mae[training_idx, i] = mae_training\n",
    "    modes_mrae[training_idx, i] = mrae_training\n",
    "    modes_mse[testing_idx, i] = mse_testing\n",
    "    modes_mae[testing_idx, i] = mae_testing\n",
    "    modes_mrae[testing_idx, i] = mrae_testing\n",
    "training_error_idx = pd.Index(training_idx, name='Timestep')\n",
    "testing_error_idx = pd.Index(testing_idx, name='Timestep')\n",
    "columns = pd.MultiIndex.from_product([TESTED_MODES_FULL_ORDER, ['MSE', 'MAE', 'MRAE']], names=['Modes', 'Metric'])\n",
    "training_error_data = np.zeros((training_error_idx.shape[0], NUMBER_OF_MODES * 3))\n",
    "testing_error_data = np.zeros((testing_error_idx.shape[0], NUMBER_OF_MODES * 3))\n",
    "training_error_data[:, ::3] = modes_mse[training_idx]\n",
    "training_error_data[:, 1::3] = modes_mae[training_idx]\n",
    "training_error_data[:, 2::3] = modes_mrae[training_idx]\n",
    "testing_error_data[:, ::3] = modes_mse[testing_idx]\n",
    "testing_error_data[:, 1::3] = modes_mae[testing_idx]\n",
    "testing_error_data[:, 2::3] = modes_mrae[testing_idx]\n",
    "training_error_df = pd.DataFrame(training_error_data, columns=columns, index=training_error_idx).sort_index()\n",
    "testing_error_df = pd.DataFrame(testing_error_data, columns=columns, index=testing_error_idx).sort_index()\n",
    "collision_full['permutation_idx'] = permutation_idx\n",
    "collision_full['training_idx'] = training_idx\n",
    "collision_full['testing_idx'] = testing_idx\n",
    "collision_full['training_error'] = training_error_df\n",
    "collision_full['testing_error'] = testing_error_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 9))\n",
    "#region LINE GRAPH\n",
    "(\n",
    "    (\n",
    "        collision_full['training_error']\n",
    "        .loc[:, (slice(None), 'MRAE')]\n",
    "        .mean()\n",
    "        .unstack()\n",
    "        * 100\n",
    "    )\n",
    "    .plot(ax=ax[0], linestyle='--', alpha=0.5)\n",
    ")\n",
    "(\n",
    "    (\n",
    "        collision_full['testing_error']\n",
    "        .loc[:, (slice(None), 'MRAE')]\n",
    "        .mean()\n",
    "        .unstack()\n",
    "        * 100\n",
    "    )\n",
    "    .plot(ax=ax[0], linestyle='-')\n",
    ")\n",
    "\n",
    "ax[0].legend([\n",
    "    'Training',\n",
    "    'Testing',\n",
    "])\n",
    "ax[0].set_xlabel(\"Número de modos\")\n",
    "ax[0].set_ylabel(\"Erro médio absoluto percentual (MAPE)\")\n",
    "#endregion\n",
    "\n",
    "#region 3D SURFACE GRAPH\n",
    "ax[1].remove()\n",
    "ax[1] = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "x, y = np.meshgrid(collision_full['testing_error'].index, TESTED_MODES_FULL_ORDER)\n",
    "z = collision_full['testing_error'].loc[:, (slice(None), 'MRAE')].values.T * 100\n",
    "ax[1].plot_surface(x, y, z)\n",
    "ax[1].set_xlabel(\"Snapshot\")\n",
    "ax[1].set_ylabel(\"Número de modos\")\n",
    "ax[1].set_zlabel(\"Erro médio absoluto percentual (MAPE)\")\n",
    "ax[1].set_title(\"Erro de teste\")\n",
    "#endregion\n",
    "\n",
    "#region AXES FORMATTTING\n",
    "ax[0].yaxis.set_major_locator(MaxNLocator())\n",
    "ax[0].yaxis.set_major_formatter(PercentFormatter())\n",
    "ax[1].zaxis.set_major_locator(MaxNLocator())\n",
    "ax[1].zaxis.set_major_formatter(PercentFormatter())\n",
    "fig.suptitle(\n",
    "    f\"\"\"Kernel PCA com kernel {kernel_name}: Simulação da colisão\n",
    "    Erro médio absoluto percentual para redução sem separação de componentes\"\"\")\n",
    "fig.tight_layout()\n",
    "#endregion\n",
    "\n",
    "#region SAVING\n",
    "\n",
    "os.makedirs(f\"{BASE_PATH}/collision\", exist_ok=True)\n",
    "fig.savefig(f\"{BASE_PATH}/collision/full_order.png\", dpi=333)\n",
    "#endregion\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spreading full order reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reordering data and rescaling energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreading_timesteps_components_last = np.transpose(spreading_timesteps_raw, (1, 0, 2))\n",
    "display(f\"Shape after transpose is: {spreading_timesteps_components_last.shape}\")\n",
    "# Flattening 2nd and 3rd dimensions\n",
    "spreading_timesteps_components_last = (\n",
    "    spreading_timesteps_components_last\n",
    "    .reshape(spreading_timesteps_components_last.shape[0], -1)\n",
    ")\n",
    "display(f\"Shape after flattening is: {spreading_timesteps_components_last.shape}\")\n",
    "spreading_full = {}\n",
    "spreading_full['original_data'] = spreading_timesteps_components_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(spreading_full['original_data'])\n",
    "display(f\"Asserting mean = 0 and std = 1 for every column\")\n",
    "assert np.allclose(scaled_data.mean(axis=0), 0)\n",
    "assert np.allclose(scaled_data.std(axis=0), 1)\n",
    "display(\"Success!\")\n",
    "spreading_full['scaled_data'] = scaled_data\n",
    "spreading_full['scaler'] = scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing every mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "NUMBER_OF_MODES = TESTED_MODES_FULL_ORDER.shape[0]\n",
    "number_of_timesteps = spreading_full['scaled_data'].shape[0]\n",
    "permutation_idx = np.random.permutation(number_of_timesteps)\n",
    "training_size = int(TRAIN_SPLIT * number_of_timesteps)\n",
    "training_idx = permutation_idx[:training_size]\n",
    "testing_idx = permutation_idx[training_size:]\n",
    "training_data = spreading_full['original_data'][training_idx]\n",
    "training_data_scaled = spreading_full['scaled_data'][training_idx]\n",
    "testing_data = spreading_full['original_data'][testing_idx]\n",
    "testing_data_scaled = spreading_full['scaled_data'][testing_idx]\n",
    "modes_mse = np.zeros((number_of_timesteps, NUMBER_OF_MODES))\n",
    "modes_mae = np.zeros_like(modes_mse)\n",
    "modes_mrae = np.zeros_like(modes_mse)\n",
    "for i, mode in enumerate(TESTED_MODES_FULL_ORDER):\n",
    "    # TRAINING DATA\n",
    "    kpca = KernelPCA(n_components=mode, random_state=42, kernel=USED_KERNEL, fit_inverse_transform=True)\n",
    "    kpca.fit(training_data_scaled)\n",
    "    reconstructed_training_scaled = kpca.inverse_transform(kpca.transform(training_data_scaled))\n",
    "    reconstructed_training = spreading_full['scaler'].inverse_transform(reconstructed_training_scaled)\n",
    "    mse_training = np.mean((training_data - reconstructed_training)**2, axis=1)\n",
    "    mae_training = np.mean(np.abs(training_data - reconstructed_training), axis=1)\n",
    "    mrae_training = np.mean(np.abs(training_data - reconstructed_training) / np.abs(training_data), axis=1)\n",
    "    # TESTING DATA\n",
    "    reconstructed_testing_scaled = kpca.inverse_transform(kpca.transform(testing_data_scaled))\n",
    "    reconstructed_testing = spreading_full['scaler'].inverse_transform(reconstructed_testing_scaled)\n",
    "    mse_testing = np.mean((testing_data - reconstructed_testing)**2, axis=1)\n",
    "    mae_testing = np.mean(np.abs(testing_data - reconstructed_testing), axis=1)\n",
    "    mrae_testing = np.mean(np.abs(testing_data - reconstructed_testing) / np.abs(testing_data), axis=1)\n",
    "    # STORING DATA\n",
    "    modes_mse[training_idx, i] = mse_training\n",
    "    modes_mae[training_idx, i] = mae_training\n",
    "    modes_mrae[training_idx, i] = mrae_training\n",
    "    modes_mse[testing_idx, i] = mse_testing\n",
    "    modes_mae[testing_idx, i] = mae_testing\n",
    "    modes_mrae[testing_idx, i] = mrae_testing\n",
    "training_error_idx = pd.Index(training_idx, name='Timestep')\n",
    "testing_error_idx = pd.Index(testing_idx, name='Timestep')\n",
    "columns = pd.MultiIndex.from_product([TESTED_MODES_FULL_ORDER, ['MSE', 'MAE', 'MRAE']], names=['Modes', 'Metric'])\n",
    "training_error_data = np.zeros((training_error_idx.shape[0], NUMBER_OF_MODES * 3))\n",
    "testing_error_data = np.zeros((testing_error_idx.shape[0], NUMBER_OF_MODES * 3))\n",
    "training_error_data[:, ::3] = modes_mse[training_idx]\n",
    "training_error_data[:, 1::3] = modes_mae[training_idx]\n",
    "training_error_data[:, 2::3] = modes_mrae[training_idx]\n",
    "testing_error_data[:, ::3] = modes_mse[testing_idx]\n",
    "testing_error_data[:, 1::3] = modes_mae[testing_idx]\n",
    "testing_error_data[:, 2::3] = modes_mrae[testing_idx]\n",
    "training_error_df = pd.DataFrame(training_error_data, columns=columns, index=training_error_idx).sort_index()\n",
    "testing_error_df = pd.DataFrame(testing_error_data, columns=columns, index=testing_error_idx).sort_index()\n",
    "spreading_full['permutation_idx'] = permutation_idx\n",
    "spreading_full['training_idx'] = training_idx\n",
    "spreading_full['testing_idx'] = testing_idx\n",
    "spreading_full['training_error'] = training_error_df\n",
    "spreading_full['testing_error'] = testing_error_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 9))\n",
    "# region LINE GRAPH\n",
    "(\n",
    "    (\n",
    "        spreading_full['training_error']\n",
    "        .loc[:, (slice(None), 'MRAE')]\n",
    "        .mean()\n",
    "        .unstack()\n",
    "        * 100\n",
    "    )\n",
    "    .plot(ax=ax[0], linestyle='--', alpha=0.5)\n",
    ")\n",
    "(\n",
    "    (\n",
    "        spreading_full['testing_error']\n",
    "        .loc[:, (slice(None), 'MRAE')]\n",
    "        .mean()\n",
    "        .unstack()\n",
    "        * 100\n",
    "    )\n",
    "    .plot(ax=ax[0], linestyle='-')\n",
    ")\n",
    "\n",
    "ax[0].legend([\n",
    "    'Training',\n",
    "    'Testing',\n",
    "])\n",
    "ax[0].set_xlabel(\"Número de modos\")\n",
    "ax[0].set_ylabel(\"Erro médio absoluto percentual (MAPE)\")\n",
    "# endregion\n",
    "\n",
    "# region 3D SURFACE GRAPH\n",
    "ax[1].remove()\n",
    "ax[1] = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "x, y = np.meshgrid(\n",
    "    spreading_full['testing_error'].index, TESTED_MODES_FULL_ORDER)\n",
    "z = spreading_full['testing_error'].loc[:,\n",
    "                                        (slice(None), 'MRAE')].values.T * 100\n",
    "ax[1].plot_surface(x, y, z)\n",
    "ax[1].set_xlabel(\"Snapshot\")\n",
    "ax[1].set_ylabel(\"Número de modos\")\n",
    "ax[1].set_zlabel(\"Erro médio absoluto percentual (MAPE)\")\n",
    "ax[1].set_title(\"Erro de teste\")\n",
    "# endregion\n",
    "\n",
    "# region AXES FORMATTTING\n",
    "ax[0].yaxis.set_major_locator(MaxNLocator())\n",
    "ax[0].yaxis.set_major_formatter(PercentFormatter())\n",
    "ax[1].zaxis.set_major_locator(MaxNLocator())\n",
    "ax[1].zaxis.set_major_formatter(PercentFormatter())\n",
    "fig.suptitle(\n",
    "    f\"\"\"Kernel PCA com kernel {kernel_name}: Simulação do espalhamento\n",
    "    Erro médio absoluto percentual para redução sem separação de componentes\"\"\")\n",
    "fig.tight_layout()\n",
    "# endregion\n",
    "\n",
    "# region SAVING\n",
    "\n",
    "os.makedirs(f\"{BASE_PATH}/spreading\", exist_ok=True)\n",
    "fig.savefig(f\"{BASE_PATH}/spreading/full_order.png\", dpi=333)\n",
    "# endregion\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
